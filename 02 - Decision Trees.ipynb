{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 02: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Wikipedia**\n",
    "\n",
    "_Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a very basic interface for a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class DecisionTree(object):\n",
    "    def __init__(self, split_scorer):\n",
    "        ...\n",
    "\n",
    "    def fit(self, data):\n",
    "        ...\n",
    "\n",
    "    def predict(self, row):\n",
    "        ...\n",
    "```\n",
    "\n",
    "The `split_scorer` will be a callback which will allow us to determine which feature to split on next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a basic API defined, we can write a few scorer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, feature):\n",
    "    \"\"\"Returns an array of non-overlapping dataframes, the union of which is the original\"\"\"\n",
    "    return [x for _, x in df.groupby(feature)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(target):\n",
    "    c = Counter(target)\n",
    "    frequencies = np.array(list(c.values()))\n",
    "    n = frequencies.sum()\n",
    "    pi = frequencies / n\n",
    "    return 1 - sum(pi**2)\n",
    "\n",
    "def gini_impurity_scorer(df, target_label):\n",
    "    \"\"\"Given a dataframe with the target label, determine the feature that gives the best split\n",
    "        using the Gini impurity as a measure\n",
    "    \"\"\"\n",
    "    target = df[target_label]\n",
    "    features = df.drop(target_label, axis=1).columns\n",
    "    \n",
    "    gini_before = gini_impurity(df[target_label])\n",
    "    \n",
    "    best_feature = None\n",
    "    \n",
    "    for feature in features:\n",
    "        df_split = split(df, feature)\n",
    "        gini_after = np.mean([gini_impurity(sub_df[target_label]) for sub_df in df_split])\n",
    "        if gini_after < gini_before:\n",
    "            best_feature = feature\n",
    "            \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target):\n",
    "    c = Counter(target)\n",
    "    frequencies = np.array(list(c.values()))\n",
    "    n = frequencies.sum()\n",
    "    pi = frequencies / n\n",
    "    return -np.sum(pi*np.log2(pi))\n",
    "\n",
    "def weighted_entropy(targets):\n",
    "    n = sum(len(t) for t in targets)\n",
    "    weights = np.array([len(t)/n for t in targets])\n",
    "    entropies = [entropy(target) for target in targets]\n",
    "    return -np.dot(weights, entropies)\n",
    "\n",
    "def information_gain(df, target_label):\n",
    "    \"\"\"Given a dataframe with the target label, determine the feature that gives the best split\n",
    "        using the Information Gain as a measure\n",
    "    \"\"\"\n",
    "    target = df[target_label]\n",
    "    features = df.drop(target_label, axis=1).columns\n",
    "    \n",
    "    parent_entropy = entropy(target)\n",
    "    \n",
    "    best_feature = None\n",
    "    ig_max = -np.inf\n",
    "    \n",
    "    for feature in features:\n",
    "        df_split = split(df, feature)\n",
    "        targets = [df_subset[target_label] for df_subset in df_split]\n",
    "        child_entropy = weighted_entropy(targets)\n",
    "        info_gain = parent_entropy - child_entropy\n",
    "        if info_gain > ig_max:\n",
    "            ig_max = info_gain\n",
    "            best_feature = feature\n",
    "            \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x\": [1,1,1,1,2,2,2,1,1,1,3], \"y\": [2,2,2,2,3,3,1,2,2,2,4]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
