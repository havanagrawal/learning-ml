{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 02: Decision Trees & Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:18.706732Z",
     "start_time": "2019-04-04T03:24:18.369857Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Wikipedia**\n",
    "\n",
    "_Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a very basic interface for a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class DecisionTree(object):\n",
    "    def __init__(self, split_scorer):\n",
    "        ...\n",
    "\n",
    "    def fit(self, data):\n",
    "        ...\n",
    "\n",
    "    def predict(self, row):\n",
    "        ...\n",
    "```\n",
    "\n",
    "The `split_scorer` will be a callback which will allow us to determine which feature to split on next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a basic API defined, we can write a few scorer functions, or metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:18.712309Z",
     "start_time": "2019-04-04T03:24:18.708873Z"
    }
   },
   "outputs": [],
   "source": [
    "def split(df, feature):\n",
    "    \"\"\"Returns an array of non-overlapping dataframes, the union of which is the original\"\"\"\n",
    "    return [x for _, x in df.groupby(feature)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Error**\n",
    "\n",
    "$$E_{CE_m} = 1 - max_{k}(\\hat{p}_{mk})$$\n",
    "\n",
    "\n",
    "\n",
    "where $\\hat{p}_{mk}$ is the proportion of training observations in the mth region that are from Kth class.\n",
    "\n",
    "This is basically the majority classifier, i.e. you predict the majority class at the leaf node.\n",
    "\n",
    "This is typically used when you prune (or that's what the book says :P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:18.717330Z",
     "start_time": "2019-04-04T03:24:18.714132Z"
    }
   },
   "outputs": [],
   "source": [
    "def classification_error(target):\n",
    "    return 1 - target.value_counts().max()/len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gini Impurity**\n",
    "\n",
    "$$G = \\sum_{k=1}^{N} p_i (1 - p_i)$$\n",
    "\n",
    "Equivalent form:\n",
    "\n",
    "$$G = 1 - \\sum_{k=1}^{N} p_i^2$$\n",
    "\n",
    "The Gini index is referred to as a measure of node purity—a small value indicates that a node contains predominantly observations from a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:18.735726Z",
     "start_time": "2019-04-04T03:24:18.720002Z"
    }
   },
   "outputs": [],
   "source": [
    "def gini_impurity(target):\n",
    "    pi = target.value_counts(normalize=True).values\n",
    "    return 1 - pi.dot(pi)\n",
    "\n",
    "def gini_impurity_scorer(df, target_label, epsilon=0.01):\n",
    "    \"\"\"Given a dataframe with the target label, determine the feature that gives the best split\n",
    "        using the Gini impurity as a measure\n",
    "    \"\"\"\n",
    "    target = df[target_label]\n",
    "    features = df.drop(target_label, axis=1).columns\n",
    "    \n",
    "    gini_before = gini_impurity(df[target_label])\n",
    "    \n",
    "    best_feature = None\n",
    "    \n",
    "    for feature in features:\n",
    "        df_split = split(df, feature)\n",
    "        gini_after = np.mean([gini_impurity(sub_df[target_label]) for sub_df in df_split])\n",
    "        if epsilon < gini_before - gini_after:\n",
    "            best_feature = feature\n",
    "            \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Gain**\n",
    "\n",
    "IG = entropy(parent) – weighted average entropy(children)\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$IG = H(T) - H(T | a)$$\n",
    "\n",
    "where entropy is given by: \n",
    "\n",
    "$H(T) = -\\sum_{i = 1}^{K} p_i log_2(p_i)$\n",
    "\n",
    "and \n",
    "\n",
    "$H(T | a) = \\sum_{t \\in T} p(t) H(t)$,\n",
    "\n",
    "i.e. the weighted sum of the entropies of the children, where they are weighted by the number of samples in that subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:18.779876Z",
     "start_time": "2019-04-04T03:24:18.737646Z"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(target):\n",
    "    pi = np.array(target.value_counts(normalize=True).tolist())\n",
    "    return -np.sum(pi*np.log2(pi))\n",
    "\n",
    "def weighted_entropy(targets):\n",
    "    n = sum(len(t) for t in targets)\n",
    "    weights = np.array([len(t)/n for t in targets])\n",
    "    entropies = [entropy(target) for target in targets]\n",
    "    return -np.dot(weights, entropies)\n",
    "\n",
    "def information_gain(df, target_label):\n",
    "    \"\"\"Given a dataframe with the target label, determine the feature that gives the best split\n",
    "        using the Information Gain as a measure\n",
    "    \"\"\"\n",
    "    target = df[target_label]\n",
    "    features = df.drop(target_label, axis=1).columns\n",
    "    \n",
    "    parent_entropy = entropy(target)\n",
    "    \n",
    "    best_feature = None\n",
    "    ig_max = -np.inf\n",
    "    \n",
    "    for feature in features:\n",
    "        df_split = split(df, feature)\n",
    "        targets = [df_subset[target_label] for df_subset in df_split]\n",
    "        child_entropy = weighted_entropy(targets)\n",
    "        info_gain = parent_entropy - child_entropy\n",
    "        if info_gain > ig_max:\n",
    "            ig_max = info_gain\n",
    "            best_feature = feature\n",
    "            \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a few scorers avaliable, we can now sketch out an initial implementation of a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:18.883754Z",
     "start_time": "2019-04-04T03:24:18.784650Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, split_scorer):\n",
    "        self.scorer = split_scorer\n",
    "        self.tree = {}\n",
    "        \n",
    "    def _majority(self, data, target_label):\n",
    "        return data[target_label].value_counts().idxmax()\n",
    "\n",
    "    def _fit(self, data, target_label, max_depth, current_depth=0):\n",
    "        features = data.drop(target_label, axis=1).columns.tolist()\n",
    "        \n",
    "        # Stopping criterion is when we have only a single class label left\n",
    "        if len(set(data[target_label])) == 1: return data[target_label].iloc[0]\n",
    "\n",
    "        # Or we don't have any features left to split on, \n",
    "        # in which case we will just predict majority:\n",
    "        if not features: return self._majority(data, target_label)\n",
    "        \n",
    "        # OR we have hit the max depth of the tree (tuning parameter)\n",
    "        if current_depth > max_depth: return self._majority(data, target_label)\n",
    "        \n",
    "        # Decide which is the best feature to split on using the split scorer\n",
    "        best_feature = self.scorer(data, target_label)\n",
    "        \n",
    "        # Split on the best feature\n",
    "        splits = split(data, best_feature)\n",
    "        \n",
    "        # Start a (sub) tree\n",
    "        current_node = {best_feature: {}}\n",
    "\n",
    "        # For every value that this feature can take, we fit a sub-tree to that subset of data \n",
    "        for val in set(data[best_feature]):\n",
    "            # We drop this feature only because this is a categorical scenario\n",
    "            # And we are not using binary splitting, but n-ary splitting.\n",
    "            data_subset = data[data[best_feature] == val].drop(best_feature, axis=1)\n",
    "\n",
    "            # Recursively fit trees for each split value of this feature\n",
    "            current_node[best_feature][val] = self._fit(data_subset, target_label, max_depth, current_depth + 1)\n",
    "\n",
    "        return current_node\n",
    "                        \n",
    "    def fit(self, data, target_label, max_depth=3):\n",
    "        self.tree = self._fit(data, target_label, max_depth=max_depth)\n",
    "\n",
    "    def predict(self, row):\n",
    "        \"\"\"The row should be a dict-like object, that is subsettable by the feature names used to train the model\"\"\"\n",
    "        current_node = self.tree\n",
    "        \n",
    "        while type(current_node) != str:\n",
    "            feature = list(current_node.keys())[0]\n",
    "            actual_value = row[feature]\n",
    "            \n",
    "            current_node = current_node[feature][actual_value]\n",
    "            \n",
    "        return current_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:18.901016Z",
     "start_time": "2019-04-04T03:24:18.885602Z"
    }
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('car.csv', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:18.931034Z",
     "start_time": "2019-04-04T03:24:18.903045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety evaluation\n",
       "0  vhigh  vhigh     2       2    small    low      unacc\n",
       "1  vhigh  vhigh     2       2    small    med      unacc\n",
       "2  vhigh  vhigh     2       2    small   high      unacc\n",
       "3  vhigh  vhigh     2       2      med    low      unacc\n",
       "4  vhigh  vhigh     2       2      med    med      unacc"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:18.943158Z",
     "start_time": "2019-04-04T03:24:18.934609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unacc', 'acc', 'vgood', 'good'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.evaluation.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:19.452549Z",
     "start_time": "2019-04-04T03:24:18.948362Z"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTree(gini_impurity_scorer)\n",
    "dt.fit(cars, 'evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the tree that was generated by printing out the decision dict. The below output should be interpreted left-to-right (as opposed to top-down in a normal visualization of a decision tree).\n",
    "\n",
    "The values for a given feature all occur at the same indentation level, so it is somewhat easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:19.469329Z",
     "start_time": "2019-04-04T03:24:19.455396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'safety': {'high': {'lug_boot': {'big': {'persons': {'2': 'unacc',\n",
      "                                                      '4': {'maint': {'high': 'acc',\n",
      "                                                                      'low': 'acc',\n",
      "                                                                      'med': 'acc',\n",
      "                                                                      'vhigh': 'acc'}},\n",
      "                                                      'more': {'maint': {'high': 'acc',\n",
      "                                                                         'low': 'acc',\n",
      "                                                                         'med': 'acc',\n",
      "                                                                         'vhigh': 'acc'}}}},\n",
      "                                  'med': {'persons': {'2': 'unacc',\n",
      "                                                      '4': {'doors': {'2': 'acc',\n",
      "                                                                      '3': 'acc',\n",
      "                                                                      '4': 'acc',\n",
      "                                                                      '5more': 'acc'}},\n",
      "                                                      'more': {'doors': {'2': 'acc',\n",
      "                                                                         '3': 'acc',\n",
      "                                                                         '4': 'acc',\n",
      "                                                                         '5more': 'acc'}}}},\n",
      "                                  'small': {'persons': {'2': 'unacc',\n",
      "                                                        '4': {'maint': {'high': 'acc',\n",
      "                                                                        'low': 'acc',\n",
      "                                                                        'med': 'acc',\n",
      "                                                                        'vhigh': 'acc'}},\n",
      "                                                        'more': {'doors': {'2': 'unacc',\n",
      "                                                                           '3': 'acc',\n",
      "                                                                           '4': 'acc',\n",
      "                                                                           '5more': 'acc'}}}}}},\n",
      "            'low': 'unacc',\n",
      "            'med': {'lug_boot': {'big': {'persons': {'2': 'unacc',\n",
      "                                                     '4': {'maint': {'high': 'acc',\n",
      "                                                                     'low': 'acc',\n",
      "                                                                     'med': 'acc',\n",
      "                                                                     'vhigh': 'acc'}},\n",
      "                                                     'more': {'maint': {'high': 'acc',\n",
      "                                                                        'low': 'acc',\n",
      "                                                                        'med': 'acc',\n",
      "                                                                        'vhigh': 'acc'}}}},\n",
      "                                 'med': {'persons': {'2': 'unacc',\n",
      "                                                     '4': {'doors': {'2': 'unacc',\n",
      "                                                                     '3': 'unacc',\n",
      "                                                                     '4': 'acc',\n",
      "                                                                     '5more': 'acc'}},\n",
      "                                                     'more': {'doors': {'2': 'unacc',\n",
      "                                                                        '3': 'acc',\n",
      "                                                                        '4': 'acc',\n",
      "                                                                        '5more': 'acc'}}}},\n",
      "                                 'small': {'persons': {'2': 'unacc',\n",
      "                                                       '4': {'maint': {'high': 'unacc',\n",
      "                                                                       'low': 'acc',\n",
      "                                                                       'med': 'acc',\n",
      "                                                                       'vhigh': 'unacc'}},\n",
      "                                                       'more': {'doors': {'2': 'unacc',\n",
      "                                                                          '3': 'unacc',\n",
      "                                                                          '4': 'unacc',\n",
      "                                                                          '5more': 'unacc'}}}}}}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dt.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:19.479957Z",
     "start_time": "2019-04-04T03:24:19.471910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unacc'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.predict({'safety': 'med', 'lug_boot': 'med', 'persons': '2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Regression Tree\n",
    "\n",
    "$\\sum_{m = 1}^{|T|} \\sum_{i: x_i \\in \\mathbb{R}_m} (y_i - \\hat{y}_{R_m}))^2 + \\alpha |T|$\n",
    "\n",
    "Use k-folds cross-validation to pick $\\alpha$:\n",
    "\n",
    "For k = 1, 2...K:  \n",
    "   1. Use recursive binary splitting to grow a large tree  \n",
    "   2. Apply cost complexity pruning\n",
    "  \n",
    "Average results for each $\\alpha$, pick $\\alpha$ to minimize RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "**Some advantages of decision trees are:**\n",
    "\n",
    "1. Simple to understand and to interpret. Trees can be visualised.\n",
    "2. Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "3. The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "4. Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.\n",
    "5. Able to handle multi-output problems.\n",
    "6. Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "7. Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "8. Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "\n",
    "**The disadvantages of decision trees include:**\n",
    "\n",
    "1. Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "2. Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "3. The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "4. There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "5. Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging / Bootstrap Aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept: Take many training sets, build separate models using each training set, and then average the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{f}_{avg}(x) = \\frac{1}{B} \\sum_{b = 1}^{B} \\hat{f}_b(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realistically, you don't have multiple training sets, and so you create B samples from your training set instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**\n",
    "\n",
    "1. Take repeated samples from single training dateset\n",
    "2. Train our model on the bth bootstrapped training set to get $\\hat{f}_b(x)$\n",
    "3. Average to get predictions\n",
    "\n",
    "These trees are grown deep (not pruned)\n",
    "\n",
    "**Prediction**\n",
    "\n",
    "Take majority vote from predicted classes from B trees.\n",
    "\n",
    "In practice, pick a large B so that the error settles down/converges (~100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:19.497051Z",
     "start_time": "2019-04-04T03:24:19.482301Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaggedDecisionTrees():\n",
    "    def __init__(self, k, sample_frac):\n",
    "        \"\"\"Create a new bagged decision tree model, with k decision trees\"\"\"\n",
    "        self.k = k\n",
    "        self.sample_frac = sample_frac\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, data, target_label):\n",
    "        for _ in range(self.k):\n",
    "            sampled_data = data.sample(frac=self.sample_frac, replace=True)\n",
    "            dt = DecisionTree(gini_impurity_scorer)\n",
    "            dt.fit(sampled_data, target_label, max_depth=2)\n",
    "            self.models.append(dt)\n",
    "            \n",
    "    def predict(self, row):\n",
    "        predictions = [model.predict(row) for model in self.models]\n",
    "        return Counter(predictions).most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:23.628247Z",
     "start_time": "2019-04-04T03:24:19.499771Z"
    }
   },
   "outputs": [],
   "source": [
    "bdt = BaggedDecisionTrees(20, 0.5)\n",
    "bdt.fit(cars, 'evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T03:24:23.635683Z",
     "start_time": "2019-04-04T03:24:23.630586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('acc', 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.predict({'safety': 'med', 'lug_boot': 'med', 'persons': '4', 'doors': '3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used an artificially low value for the sampling fraction, as well as the number of trees, just to illustrate that a majority vote is being taken for the prediction.\n",
    "\n",
    "We can see that out of the 20 classifiers that we trained, 12 of them voted for 'acc', and so that is our final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very similar to bagging, but decorrelates the trees.\n",
    "\n",
    "You randomly sample from the set of predictions $X = {X_1, X_2, X_3 \\ldots X_p}$, say $m = {X_1, X_4, X_6 \\ldots}$.\n",
    "\n",
    "The rationale is that if you have a very strong predictor $X_k$, then in regular bagging it will end up in most of the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m \\simeq \\sqrt{p}$ is an ideal choice.\n",
    "\n",
    "$m = p$ is just regular bagging.\n",
    "\n",
    "In general, small m is helpful when you have a large number of correlated predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
